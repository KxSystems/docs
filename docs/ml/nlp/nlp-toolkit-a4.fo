<?xml version="1.0"?>
<fo:root xmlns:fo="http://www.w3.org/1999/XSL/Format" xmlns:db="http://docbook.org/ns/docbook" xmlns:rx="http://www.renderx.com/XSL/Extensions" xmlns:xlink="http://www.w3.org/1999/xlink"><rx:meta-info><rx:meta-field name="author" value="&#10;        Fionnuala&#10;        Carr&#10;      "/><rx:meta-field name="creator" value="Kx Systems"/><rx:meta-field name="title" value="Natural-language processing toolkit"/><rx:meta-field name="keywords" value="Kx, Kx Systems, kdb+, machine learning, ml, nlp, sentiment analysis"/></rx:meta-info><rx:outline><rx:bookmark internal-destination="natural-language-processing"><rx:bookmark-label>Natural-language processing</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="preparing-text"><rx:bookmark-label>Preparing text</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="feature-vectors"><rx:bookmark-label>Feature vectors</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="comparisons"><rx:bookmark-label>Comparisons</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="clustering"><rx:bookmark-label>Clustering</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="finding-outliers-and-representative-documents"><rx:bookmark-label>Finding outliers, and representative documents</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="explaining-similarities"><rx:bookmark-label>Explaining similarities</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="sentiment-analysis"><rx:bookmark-label>Sentiment analysis</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="importing-and-parsing-mbox-files"><rx:bookmark-label>Importing and parsing MBOX files</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="parsing-emails-from-a-string-format"><rx:bookmark-label>Parsing emails from a string format</rx:bookmark-label></rx:bookmark><rx:bookmark internal-destination="useful-functions"><rx:bookmark-label>Useful functions</rx:bookmark-label></rx:bookmark></rx:outline><fo:layout-master-set><fo:simple-page-master master-name="cover-page" page-width="210mm" page-height="297mm"><fo:region-body margin-top="0" margin-bottom="0" margin-left="0" margin-right="0" background-color="#eeeded"/></fo:simple-page-master><fo:simple-page-master master-name="toc" page-width="210mm" page-height="297mm" margin-top="30pt" margin-bottom="30pt" margin-left="45pt" margin-right="45pt"><fo:region-body margin-top="48pt" margin-bottom="32pt" margin-left="60pt" margin-right="60pt"/><fo:region-before extent="30pt"/><fo:region-after extent="30pt"/></fo:simple-page-master><fo:simple-page-master master-name="standard-page" page-width="210mm" page-height="297mm" margin-top="30pt" margin-bottom="30pt" margin-left="45pt" margin-right="45pt"><fo:region-body margin-top="48pt" margin-bottom="32pt" margin-left="40pt" margin-right="40pt"/><fo:region-before extent="30pt"/><fo:region-after extent="30pt"/></fo:simple-page-master></fo:layout-master-set><fo:page-sequence master-reference="cover-page" font-family="STIX2" initial-page-number="1" language="en" country="gb"><fo:flow flow-name="xsl-region-body"><fo:block-container absolute-position="absolute" top="0mm" right="0mm" width="2in" height="2in"><fo:block text-align="right"><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/diamond-white.png)" content-width="40mm" scaling="uniform"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" bottom="0mm" left="0mm" width="2.5in" height="30mm"><fo:block><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/diamond-bottom-left-white.png)" content-width="50mm" scaling="uniform"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" bottom="0mm" right="0mm" width="4in" height="50mm"><fo:block text-align="right"><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/kx-cover.png)" content-width="75mm"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="35mm" left="15mm"><fo:block color="#0070cd" font-family="Proxima Nova" font-size="60pt" font-weight="bold" letter-spacing="-4pt">kx</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="40mm" left="40mm"><fo:block><fo:external-graphic src="url(/Users/sjt/Projects/kx/github/StephenTaylor-Kx/mkdocs2pdf/img/its-about-time.png)"/></fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="80mm" left="35mm"><fo:block font-family="Proxima Nova" color="#0070cd" font-size="18pt" font-weight="400">
							Technical Whitepaper
						</fo:block><fo:block margin-top="9pt" margin-right="30mm" line-height="1.4" font-size="24pt" font-weight="400">Natural-language processing toolkit</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="170mm" left="35mm"><fo:block line-height="16pt" font-weight="bold">Date</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="170mm" left="55mm" width="100mm"><fo:block line-height="16pt" text-align="left">May 2018</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="180mm" left="35mm"><fo:block line-height="16pt" font-weight="bold">Author</fo:block></fo:block-container><fo:block-container absolute-position="absolute" top="180mm" left="55mm" width="100mm"><fo:block line-height="16pt" text-align="left">
        Fionnuala
        Carr
      </fo:block></fo:block-container><fo:block> </fo:block></fo:flow></fo:page-sequence><fo:page-sequence master-reference="toc" font-family="STIX2" language="en" country="gb"><fo:static-content flow-name="xsl-region-before"><fo:block text-align-last="justify" font-family="Proxima Nova" color="gray"><fo:inline font-size="9pt">Natural-language processing toolkit</fo:inline><fo:leader leader-pattern="space"/><fo:inline color="#0070cd" font-size="18pt" font-weight="bold" letter-spacing="-2pt">kx</fo:inline></fo:block></fo:static-content><fo:static-content flow-name="xsl-region-after"><fo:block color="gray" text-align="right"><fo:inline font-family="Proxima Nova" font-size="9pt" letter-spacing="2pt"><fo:page-number/></fo:inline></fo:block></fo:static-content><fo:flow flow-name="xsl-region-body"><fo:block break-before="page"><fo:block font-size="14pt" text-align="center" margin-top="36pt" margin-bottom="36pt">Contents</fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219962575320">Natural-language processing <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219962575320"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219962577832">Preparing text <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219962577832"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219962651512">Feature vectors <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219962651512"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219962690696">Comparisons <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219962690696"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219962724344">Clustering <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219962724344"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219963617496">Finding outliers, and representative documents <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219963617496"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219963632616">Explaining similarities <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219963632616"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219963639688">Sentiment analysis <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219963639688"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219963647480">Importing and parsing MBOX files <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219963647480"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219963678408">Parsing emails from a string format <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219963678408"/></fo:basic-link></fo:block><fo:block margin-bottom="6pt" text-align-last="justify"><fo:basic-link internal-destination="idp140219963683992">Useful functions <fo:leader leader-pattern="dots"/> <fo:page-number-citation ref-id="idp140219963683992"/></fo:basic-link></fo:block></fo:block></fo:flow></fo:page-sequence><fo:page-sequence master-reference="standard-page" font-family="STIX2" language="en" country="gb"><fo:static-content flow-name="xsl-region-before"><fo:block text-align-last="justify" font-family="Proxima Nova" color="gray"><fo:inline font-size="9pt">Natural-language processing toolkit</fo:inline><fo:leader leader-pattern="space"/><fo:inline color="#0070cd" font-size="18pt" font-weight="bold" letter-spacing="-2pt">kx</fo:inline></fo:block></fo:static-content><fo:static-content flow-name="xsl-footnote-separator"><fo:block><fo:leader leader-pattern="rule" rule-thickness=".5pt" leader-length="50%"/></fo:block></fo:static-content><fo:static-content flow-name="xsl-region-after"><fo:block color="gray" text-align="right"><fo:inline font-family="Proxima Nova" font-size="9pt" letter-spacing="2pt"><fo:page-number/></fo:inline></fo:block></fo:static-content><fo:flow flow-name="xsl-region-body"><fo:block id="natural-language-processing">
  <fo:block id="idp140219962575320" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Natural-language processing</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Natural-language processing (NLP) can be used to answer a variety of
    questions about unstructured text data, as well as facilitating
    open-ended exploration.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    It can be applied to datasets such as emails, online articles and
    comments, tweets, or novels. Although the source is text,
    transformations are applied to convert this data to vectors,
    dictionaries and symbols which can be handled very effectively by q.
    Many operations such as searching, clustering, and keyword
    extraction can all be done using very simple data structures, such
    as feature vectors and bag-of-words representations.
  </fo:block>
</fo:block><fo:block id="preparing-text">
  <fo:block id="idp140219962577832" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Preparing text</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Operations can be pre-run on a corpus, with the results cached to a
    table, which can be persisted.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Operations undertaken to parse the dataset:
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            operation
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            effect
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Tokenization
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            splits the words; e.g. <fo:inline font-family="Pragmata Pro">John’s</fo:inline> becomes
            <fo:inline font-family="Pragmata Pro">John</fo:inline> as one token, and
            <fo:inline font-family="Pragmata Pro">‘s</fo:inline> as a second
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Sentence detection
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            characters at which a sentence starts and ends
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Part of speech tagger
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            parses the sentences into tokens and gives each token a
            label e.g. <fo:inline font-family="Pragmata Pro">lemma</fo:inline>, <fo:inline font-family="Pragmata Pro">pos</fo:inline>,
            <fo:inline font-family="Pragmata Pro">tag</fo:inline> etc.
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            Lemmatization
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            converts to a base form e.g. <fo:inline font-family="Pragmata Pro">ran</fo:inline> (verb)
            to <fo:inline font-family="Pragmata Pro">run</fo:inline> (verb)
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block id="nlp.newparser">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.newParser</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">Creates a parser</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.newParser[spacymodel;fields]</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          <fo:inline font-family="Pragmata Pro">spacymodel</fo:inline> is a
          <fo:basic-link external-destination="url('https://spacy.io/usage/models')"><fo:inline color="#0070cd">model or
          language</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">1</fo:inline><fo:footnote-body><fo:block font-size="8pt">1.
									https://spacy.io/usage/models</fo:block></fo:footnote-body></fo:footnote> (symbol)
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          <fo:inline font-family="Pragmata Pro">fields</fo:inline> is the field/s you want in the
          output (symbol atom or vector)
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      returns a function to parse the text.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The optional fields are:
    </fo:block>
    <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              field
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              type
            </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
              content
            </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">text</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              list of characters
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              original text
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">tokens</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              list of symbols
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              the tokenized text
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">sentChars</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              list of lists of longs
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              indexes of start and end of sentences
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">sentIndices</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              list of integers
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              indexes of the first token of each sentences
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">pennPOS</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              list of symbols
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              the Penn Treebank tagset
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">uniPOS</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              list of symbols
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              the Universal tagset
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">lemmas</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              list of symbols
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              the base form of the word
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">isStop</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              boolean
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              is the token part of the stop list?
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">likeEmail</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              boolean
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              does the token resembles an email?
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">likeURL</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              boolean
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              does the token resembles a URL?
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">likeNumber</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              boolean
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              does the token resembles a number?
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">keywords</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              list of dictionaries
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              significance of each term
            </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              <fo:inline font-family="Pragmata Pro">starts</fo:inline>
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              long
            </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
              index that a token starts at
            </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The resulting function is applied to a list of strings.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Parsing the novel <fo:inline font-style="italic">Moby Dick</fo:inline>:
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
/ creating a parsed table  
fields:`text`tokens`lemmas`pennPOS`isStop`sentChars`starts`sentIndices`keywords
myparser:.nlp.newParser[`en;fields] 
corpus:myparser mobyDick 
cols corpus
`tokens`lemmas`pennPOS`isStop`sentChars`starts`sentIndices`keywords`text
</fo:block>
  </fo:block>
  <fo:block id="finding-part-of-speech-tags-in-a-corpus">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Finding part-of-speech tags in a corpus</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      This is a quick way to find all of the nouns, adverbs, etc. in a
      corpus. There are two types of part-of-speech (POS) tags you can
      find:
      <fo:basic-link external-destination="url('https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html')"><fo:inline color="#0070cd">Penn
      Tree tags</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">2</fo:inline><fo:footnote-body><fo:block font-size="8pt">2.
									https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</fo:block></fo:footnote-body></fo:footnote> and
      <fo:basic-link external-destination="url('http://universaldependencies.org/docs/en/pos/all.html')"><fo:inline color="#0070cd">Universal
      Tree tags</fo:inline></fo:basic-link><fo:footnote><fo:inline font-size="8pt" alignment-baseline="hanging">3</fo:inline><fo:footnote-body><fo:block font-size="8pt">3.
									http://universaldependencies.org/docs/en/pos/all.html</fo:block></fo:footnote-body></fo:footnote>.
    </fo:block>
    <fo:block id="nlp.findposruns">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.findPOSRuns</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Runs of tokens whose POS tags are in the set
        passed</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax:
        <fo:inline font-family="Pragmata Pro">.nlp.findPOSRuns[tagtype;tags;document]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">tagtype</fo:inline> is <fo:inline font-family="Pragmata Pro">uniPos</fo:inline> or
            <fo:inline font-family="Pragmata Pro">pennPos</fo:inline>
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">tags</fo:inline> is one or more POS tags (symbol atom
            or vector)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">document</fo:inline> is parsed text (dictionary)
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns a general list:
      </fo:block>
      <fo:list-block provisional-distance-between-starts=".7cm" provisional-label-separation="0.3cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">1.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            text of the run (symbol vector)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">2.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            indexes of the first occurrence of each token (long vector)
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Importing a novel from a plain text file, and finding all the
        proper nouns in the first chapter of <fo:inline font-style="italic">Moby
        Dick</fo:inline>:
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
fields:`text`tokens`lemmas`pennPOS`isStop`sentChars`starts`sentIndices`keywords
q)myparser:.nlp.parser.i.newParser[`en;fields] 
q)corpus:myparser mobyDick 

q).nlp.findPOSRuns[`pennPOS;`NNP`NNPS;corpus 0][;0]
`loomings`ishmael`november`cato`manhattoes`circumambulate`sabbath`go`corlears`hook`coenties
</fo:block>
    </fo:block>
  </fo:block>
</fo:block><fo:block id="feature-vectors">
  <fo:block id="idp140219962651512" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Feature vectors</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    We can generate a dictionary of descriptive terms, which consist of
    terms and their associated weights. These dictionaries are called
    <fo:inline font-style="italic">feature vectors</fo:inline> and they are very useful as
    they give a uniform representation that can describe words,
    sentences, paragraphs, documents, collections of documents,
    clusters, concepts and queries.
  </fo:block>
  <fo:block id="calculating-feature-vectors-for-documents">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Calculating feature vectors for documents</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The values associated with each term in a feature vector are how
      significant that term is as a descriptor of the entity. For
      documents, this can be calculated by comparing the frequency of
      words in that document to the frequency of words in the rest of
      the corpus.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Sorting the terms in a feature vector by their significance, you
      get the keywords that distinguish a document most from the corpus,
      forming a terse summary of the document. This shows the most
      significant terms in the feature vector for one of Enron CEO Jeff
      Skilling’s email’s describing a charity bike ride.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      TF-IDF is an algorithm that weighs a term’s frequency (TF) and its
      inverse document frequency (IDF). Each word or term has its
      respective TF and IDF score. The product of the TF and IDF scores
      of a term is called the TF-IDF weight of that term.
    </fo:block>
    <fo:block id="nlp.tfidf">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.TFIDF</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">TF-IDF scores for all terms in the document</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.TFIDF x</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is a table of documents, returns for
        each document, a dictionary with the tokens as keys, and
        relevance as values.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Extract a specific document and find the most significiant words
        in that document:
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)queriedemail:jeffcorpus[where jeffcorpus[`text] like "*charity bike*"]`text;
q)5#desc .nlp.TFIDF[jeffcorpus]1928
bikers   | 17.7979
biker    | 17.7979
strenuous| 14.19154
route    | 14.11932
rode     | 14.11136
</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        In cases where the dataset is more similar to a single document
        than a collection of separate documents, a different algorithm
        can be used. This algorithm is taken from Carpena, P., et al.
        “Level statistics of words: Finding keywords in literary texts
        and symbolic sequences.”. The idea behind the algorithm is that
        more important words occur in clusters and less important words
        follow a random distribution.
      </fo:block>
    </fo:block>
    <fo:block id="nlp.keywordscontinuous">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.keywordsContinuous</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">For an input which is conceptually a single document,
        such as a book, this will give better results than
        TF-IDF</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.keywordsContinuous x</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is a table of documents, returns a
        dictionary where the keys are keywords and the values are their
        significance.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Treating all of <fo:inline font-style="italic">Moby Dick</fo:inline> as a single
        document, the most significant keywords are
        <fo:inline font-style="italic">Ahab</fo:inline>, <fo:inline font-style="italic">Bildad</fo:inline>,
        <fo:inline font-style="italic">Peleg</fo:inline> (the three captains on the boat) and
        <fo:inline font-style="italic">whale</fo:inline>.
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)10#keywords:.nlp.keywordsContinuous corpus
ahab     | 65.23191
peleg    | 52.21875
bildad   | 46.56072
whale    | 42.72953
stubb    | 38.11739
queequeg | 35.34769
steelkilt| 33.96713
pip      | 32.90067
starbuck | 32.05286
thou     | 32.05231
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="calculating-feature-vectors-for-words">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Calculating feature vectors for words</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The feature vector for a word can be calculated as a collection of
      how well other words predict the given keyword. The weight given
      to these words is a function of how much higher the actual
      co-occurrence rate is from the expected co-occurrence rate the
      terms would have if they were randomly distributed.
    </fo:block>
    <fo:block id="nlp.findrelatedterms">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.findRelatedTerms</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Feature vector for a term</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.findRelatedTerms[x;y]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">x</fo:inline> is a list of documents
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">y</fo:inline> is a symbol which is the token for
            which to find related terms
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns a dictionary of the related tokens and their relevances.
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.findRelated[corpus;`captain]
peleg | 1.653247
bildad| 1.326868
ahab  | 1.232073
ship  | 1.158671
cabin | 0.9743517
</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Phrases can be found by looking for runs of words with an
        above-average significance to the query term.
      </fo:block>
    </fo:block>
    <fo:block id="nlp.extractphrases">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.extractPhrases</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Runs of tokens that contain the term where each
        consecutive word has an above-average co-occurrence with the
        term</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.extractPhrases[corpus;term]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">corpus</fo:inline> is a subcorpus (table)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">term</fo:inline> is the term to extract phrases
            around (symbol)
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns a dictionary with phrases as the keys and their
        relevance as the values.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Search for the phrases that contain <fo:inline font-family="Pragmata Pro">captain</fo:inline>
        and see which phrase has the largest occurrence; we find
        <fo:inline font-family="Pragmata Pro">captain ahab</fo:inline> occurs most often in the book:
        31 times.
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.extractPhrases[corpus;`captain]  
"captain ahab"        | 31
"captain peleg"       | 12
"captain bildad"      | 7
"captain sleet"       | 5
"stranger captain"    | 4
"said the captain"    | 3
"sea-captain"         | 2
"whaling captain"     | 2
"captain's cabin"     | 2
"captain ahab,\" said"| 2
"captain pollard"     | 2
"captain d'wolf"      | 2
"way, captain"        | 2
</fo:block>
    </fo:block>
  </fo:block>
</fo:block><fo:block id="comparisons">
  <fo:block id="idp140219962690696" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Comparisons</fo:block>
  <fo:block id="comparing-feature-vectors">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Comparing feature vectors</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      A vector can be thought of either as
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          the co-ordinates of a point
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          describing a line segment from the origin to a point
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The view of a vector as a line segment starting at the origin is
      useful, as any two vectors will have an angle between them,
      corresponding to their similarity, as calculated by cosine
      similarity.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The <fo:inline font-style="italic">cosine similarity</fo:inline> of two vectors is the
      dot product of two vectors over the product of their magnitudes.
      It is a standard distance metric for comparing documents.
    </fo:block>
  </fo:block>
  <fo:block id="comparing-corpora">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Comparing corpora</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      A quick way to compare corpora is to find words common to the
      whole dataset, but with a strong affinity to only one corpus. This
      is a function of how much higher their frequency is in that corpus
      than in the dataset.
    </fo:block>
    <fo:block id="nlp.comparecorpora">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.compareCorpora</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Terms’ comparative affinities to two
        corpora</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.compareCorpora[corpus1;corpus2]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where <fo:inline font-family="Pragmata Pro">corpus1</fo:inline> and <fo:inline font-family="Pragmata Pro">corpus2</fo:inline>
        are tables of lists of documents, returns a dictionary of terms
        and their affinity for <fo:inline font-family="Pragmata Pro">corpus2</fo:inline> over
        <fo:inline font-family="Pragmata Pro">corpus1</fo:inline>.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Enron CEO Jeff Skillings was a member of the Beta Theta Pi
        fraternity at Southern Methodist University (SMU). If we want to
        find secret fraternity code words used by the Betas, we can
        compare his fraternity emails (those containing
        <fo:inline font-style="italic">SMU</fo:inline> or <fo:inline font-style="italic">Betas</fo:inline>) to his
        other emails.
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)fraternity:jeffcorpus i:where (jeffcorpus[`text] like "*Betas*")|jeffcorpus[`text] like "*SMU*"
q)remaining:jeffcorpus til[count jeffcorpus]except i
q)summaries:key each 10#/:.nlp.compareCorpora[fraternity;remaining]
q)summaries 0  / summary of the fraternity corpus
`beta`homecoming`betas`smu`yahoo`groups`tent`reunion`forget`crowd
q)summaries 1  / summary of the remaining corpus
`enron`jeff`business`information`please`market`services`energy`management`company
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="comparing-documents">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Comparing documents</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      This function allows you to calculate the similarity of two
      different documents. It finds the keywords that are present in
      both the corporas, and calculates the cosine similarity.
    </fo:block>
    <fo:block id="nlp.comparedocs">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.compareDocs</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Cosine similarity of two documents</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.compareDocs[dict1;dict2]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where <fo:inline font-family="Pragmata Pro">dict1</fo:inline> and <fo:inline font-family="Pragmata Pro">dict2</fo:inline> are
        dictionaries that consist of the document‘s keywords, returns
        the cosine similarity of two documents.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Given the queried email defined above, and a random email from
        the corpus, we can calculate the cosine similarity between them.
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)queryemail2:jeffcorpus[rand count jeffcorpus]
q).nlp.compareDocs[queryemail`keywords;email2`keywords]
0.1163404
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="comparing-documents-to-corpus">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Comparing documents to corpus</fo:block>
    <fo:block id="nlp.comparedoctocorpus">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.compareDocToCorpus</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Cosine similarity between a document and other
        documents in the corpus</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.compareDocToCorpus[keywords;idx]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">keywords</fo:inline> is a list of dictionaries of
            keywords and coefficients
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">idx</fo:inline> is the index of the feature vector to
            compare with the rest of the corpus
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns as a float the document’s significance to the rest of
        the corpus.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Comparing the first chapter with the rest of the book:
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.compareDocToCorpus[corpus`keywords;0]
0.03592943 0.04720108 0.03166343 0.02691693 0.03363885 0.02942622 0.03097797 0.04085023 0.04321152 0.02024251 0.02312604 0.03604447 0.02903568 0.02761553 0.04809854 0.03634777 0.02755392 0.02300291
</fo:block>
    </fo:block>
  </fo:block>
</fo:block><fo:block id="clustering">
  <fo:block id="idp140219962724344" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Clustering</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The NLP library contains a variety of clustering algorithms, with
    different parameters and performance characteristics. Some of these
    are very fast on large data sets, though they look only at the most
    salient features of each document, and will create many small
    clusters. Others, such as bisecting k-means, look at all features
    present in the document, and allow you to specify the number of
    clusters. Other parameters can include a threshold for the minimum
    similarity to consider, or how many iterations the algorithm should
    take to refine the clusters. Some clustering algorithms are
    randomized, and will produce different clusters every time they are
    run. This can be very useful, as a data set will have many possible,
    equally valid, clusterings. Some algorithms will put every document
    in a cluster, whereas others will increase cluster cohesion by
    omitting outliers.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Clusters can be summarized by their centroids, which are the sum of
    the feature vectors of all the documents they contain.
  </fo:block>
  <fo:block id="markox-cluster-algorithm">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Markox Cluster algorithm</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      MCL clustering, which takes document similarity as its only
      parameter other than the documents. This algorithm first generates
      an undirected graph of documents by classifying document pairs as
      related or unrelated, depending on whether their similarity
      exceeds a given threshold. If they are related, an edge will be
      created between these documents. Then it runs a graph-clustering
      algorithm on the dataset.
    </fo:block>
    <fo:block id="nlp.cluster.mcl">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.cluster.MCL</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Cluster a subcorpus using graph clustering</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax:
        <fo:inline font-family="Pragmata Pro">.nlp.cluster.similarity[document;min;sample]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">document</fo:inline> is a table of documents
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">min</fo:inline> is the minimum similarity (float)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">sample</fo:inline> is whether a sample of
            <fo:inline font-family="Pragmata Pro">sqrt[n]</fo:inline> documents is to be used (boolean)
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns as a list of longs the document’s indexes, grouped into
        clusters.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Cluster 2603 of Jeff Skillings emails, creating 398 clusters
        with the minimum threshold at 0.25:
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)clusterjeff:.nlp.cluster.similarity[jeffcorpus;0.25;0b]
q)count clusterjeff
398
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="summarizing-cluster-algorithm">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Summarizing Cluster algorithm</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      This clustering algorithm finds the top ten keywords in each
      document, finds the average of these keywords and determines the
      top keyword. This is set to be the centroid and therefore finds
      the closest document. This process is repeated until the number of
      clusters are found.
    </fo:block>
    <fo:block id="nlp.cluster.summarize">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.cluster.summarize</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">A clustering algorithm that works like many
        summarizing algorithms, by finding the most representative
        elements, then subtracting them from the centroid and iterating
        until the number of clusters has been reached</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax:
        <fo:inline font-family="Pragmata Pro">.nlp.cluster.summarize[docs;noOfClusters]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">docs</fo:inline> is a list of documents or document
            keywords (table or list of dictionaries)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">noOfClusters</fo:inline> is the number of clusters to
            return (long)
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns the documents’ indexes, grouped into clusters.
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.cluster.summarize[jeffcorpus;30]

0 31 47 127 361 431 513 615 724 786 929 933 1058..
1 40 44 189 507 514 577 585 746 805 869 1042.. 
2 3 4 6 7 9 10 13 16 17 19 20 22 23 24 28 33 34..
5 27 30 39 393 611 641 654 670 782 820 1358..
8 73 147 427 592 660 743 794 850
11 26 113 236 263 280 281 340 391 414 429 478..
12 14 38 43 49 52 89 173 232 278 325 328 
15 18 21 25 32 45 100 119 168 202 285 298..
29 159 386 430 459 499 508 597 659 731 
68 83 105 132 141 152 177 182 185 226 257.. 
78 91 219 225 231 239 244 255 401 477 524 551..
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="k-means-clustering">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">K-means clustering</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Given a set of documents, K-means clustering aims to partition the
      documents into a number of sets. Its objective is to minimize the
      residual sum of squares, a measure of how well the centroids
      represent the members of their clusters.
    </fo:block>
    <fo:block id="nlp.cluster.kmeans">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.cluster.kmeans</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">K-means clustering for documents</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.cluster.kmeans[docs;k;iters]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">docs</fo:inline> is a table or a list of dictionaries
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">k</fo:inline> is the number of clusters to return
            (long)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">iters</fo:inline> is the number of times to iterate
            the refining step (long)
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns the document’s indexes, grouped into clusters.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Partition <fo:inline font-style="italic">Moby Dick</fo:inline> into 15 clusters; we
        find there is one large cluster present in the book:
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)clusters:.nlp.cluster.kmeans[corpus;15;30]
q)count each clusters
32 9 13 9 12 5 12 8 6 8 7 11 11 5 2
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="bisecting-k-means">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Bisecting K-means</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Bisecting K-means adopts the K-means algorithm and splits a
      cluster in two. This algorithm is more efficient when
      <fo:inline font-style="italic">k</fo:inline> is large. For the K-means algorithm, the
      computation involves every data point of the data set and
      <fo:inline font-style="italic">k</fo:inline> centroids. On the other hand, in each
      bisecting step of Bisecting K-means, only the data points of one
      cluster and two centroids are involved in the computation. Thus
      the computation time is reduced. Secondly, Bisecting K-means
      produce clusters of similar sizes, while K-means is known to
      produce clusters of widely differing sizes.
    </fo:block>
    <fo:block id="nlp.cluster.bisectingkmeans">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.cluster.bisectingKmeans</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">The Bisecting K-means algorithm uses K-means
        repeatedly to split the most cohesive clusters into two
        clusters</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax:
        <fo:inline font-family="Pragmata Pro">.nlp.cluster.bisectingKmeans[docs;k;iters]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">docs</fo:inline> is a list of document keywords
            (table or list of dictionaries)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">k</fo:inline> is the number of clusters (long)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">iters</fo:inline> is the number of times to iterate
            the refining step
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns, as a list of lists of longs, the documents’ indexes,
        grouped into clusters.
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)count each .nlp.cluster.bisectingKMeans[corpus;15;30]
8 5 13 5 12 8 10 10 1 12 5 15 1 37 8
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="radix-algorithm">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Radix algorithm</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The Radix clustering algorithms are a set of non-comparison,
      binning-based clustering algorithms. Because they do no
      comparisons, they can be much faster than other clustering
      algorithms. In essence, they cluster via topic modeling, but
      without the complexity.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Radix clustering is based on the observation that Bisecting
      K-means clustering gives the best cohesion when the centroid
      retains only its most significant dimension, and inspired by the
      canopy-clustering approach of pre-clustering using a very cheap
      distance metric.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      At its simplest, Radix clustering just bins on most significant
      term. A more accurate version uses the most significant
      <fo:inline font-style="italic">n</fo:inline> terms in each document in the corpus as
      bins, discarding infrequent bins. Related terms can also be
      binned, and documents matching some percent of a bins keyword go
      in that bin.
    </fo:block>
    <fo:block id="hard-clustering">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Hard Clustering</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Hard Clustering means that each datapoint belongs to a cluster
        completely or not.
      </fo:block>
    </fo:block>
    <fo:block id="nlp.cluster.fastradix">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.cluster.fastradix</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Uses the Radix clustering algorithm and bins by the
        most significant term</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax:
        <fo:inline font-family="Pragmata Pro">.nlp.cluster.fastradix[docs;numOfClusters]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">docs</fo:inline> is a list of documents or document
            keywords (table or a list of dictionaries)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">numOfClusters</fo:inline> is the number of clusters
            (long)
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns a list of list of longs, the documents’ indexes, grouped
        into clusters.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Group Jeff Skilling’s emails into 60 clusters:
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)count each .nlp.cluster.radix1[jeffcorpus;60]
15 14 10 9 8 13 9 8 8 6 5 6 6 8 5 6 5 4 4 4 4 4 4 8 4 5 4 4 5 4 4 4 3 3 3 3 3..
</fo:block>
    </fo:block>
    <fo:block id="soft-clustering">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Soft Clustering</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        In Soft Clustering, a probability or likelihood of a data point
        to be in a clusters is assigned. This mean that some clusters
        can overlap.
      </fo:block>
    </fo:block>
    <fo:block id="nlp.cluster.radix">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.cluster.radix</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Uses the Radix clustering algorithm and bins are taken
        from the top 3 terms of each document</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax:
        <fo:inline font-family="Pragmata Pro">.nlp.cluster.radix[docs;numOfClusters]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">docs</fo:inline> is a list of documents or document
            keywords (table or a list of dictionaries)
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">numOfClusters</fo:inline> is the number of clusters
            (long), which should be large to cover the substantial
            amount of the corpus, as the clusters are small
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns the documents’ indexes (as a list of longs), grouped
        into clusters.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Group Jeff Skilling’s emails into 60 clusters:
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)count each .nlp.cluster.radix2[jeffcorpus;60]
9 7 6 7 10 12 6 5 5 5 6 8 6 5 8 5 6 5 5 5 6 7 5 5 5 6 9 6 5 5 9 5 5 8 17 7 37.
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="cluster-cohesion">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Cluster cohesion</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The cohesiveness of a cluster is a measure of how similar the
      documents are within that cluster. It is calculated as the mean
      sum-of-squares error, which aggregates each document’s distance
      from the centroid. Sorting by cohesiveness will give very focused
      clusters first.
    </fo:block>
    <fo:block id="nlp.cluster.mse">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.cluster.MSE</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Cohesiveness of a cluster as measured by the mean
        sum-of-squares error</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax: <fo:inline font-family="Pragmata Pro">.nlp.cluster.MSE x</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is a list of dictionaries which are a
        document’s keyword field, returns as a float the cohesion of the
        cluster.
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)/16 emails related to donating to charity
q)charityemails:jeffcorpus where jeffcorpus[`text] like "*donate*"
q).nlp.cluster.MSE charityemails`keywords
0.1177886

q)/10 emails chosen at random
q).nlp.cluster.MSE (-10?jeffcorpus)`keywords
0.02862244
</fo:block>
    </fo:block>
  </fo:block>
  <fo:block id="grouping-documents-to-centroids">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Grouping documents to centroids</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      When you have a set of centroids and you would like to find out
      which centroid is closest to the documents, you can use this
      function.
    </fo:block>
    <fo:block id="nlp.cluster.groupbycentroid">
      <fo:block font-size="12pt" font-weight="bold" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.cluster.groupByCentroid</fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        <fo:inline font-style="italic">Documents matched to their nearest centroid</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Syntax:
        <fo:inline font-family="Pragmata Pro">.nlp.cluster.matchDocswithCentroid[centroid;docs]</fo:inline>
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Where
      </fo:block>
      <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">centroid</fo:inline> is a list of the centroids as
            keyword dictionaries
          </fo:block>
        </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
          <fo:block line-height="16pt" space-after="9pt" text-align="justify">
            <fo:inline font-family="Pragmata Pro">documents</fo:inline> is a list of document feature
            vectors
          </fo:block>
        </fo:list-item-body></fo:list-item></fo:list-block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        returns, as a list of lists of longs, document indexes where
        each list is a cluster.
      </fo:block>
      <fo:block line-height="16pt" space-after="9pt" text-align="justify">
        Matches the first centroid of the clusters with the rest of the
        corpus:
      </fo:block>
      <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.cluster.groupByCentroids[[corpus clusters][0][`keywords];corpus`keywords]
0 23 65 137
1 5 14 45 81
2 6 7 13 15 16 17 19 20 21 26 27 31 40 44 47 48 49 50 54 57 58 62 63 66 67 68..
3 9 10
,4
8 51 55 95 96 108 112 117 129 132 136 146 148
11 12
,18
22 25
,24
28 53 61 72 82 83 86 91 113 130 147
,29
,30
32 33 79 98 104 105 107 131
34 97
35 37 38 39 41 42
36 133 149
43 60 64 74 106 115
</fo:block>
    </fo:block>
  </fo:block>
</fo:block><fo:block id="finding-outliers-and-representative-documents">
  <fo:block id="idp140219963617496" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Finding outliers, and representative documents</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The <fo:inline font-style="italic">centroid</fo:inline> of a collection of documents is
    the average of their feature vectors. As such, documents close to
    the centroid are representative, while those far away are the
    outliers. Given a collection of documents, finding outliers can be a
    quick way to find interesting documents, those that have been
    mis-clustered, or those not relevant to the collection.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The emails of former Enron CEO Ken Lay contain 1124 emails with a
    petition. Nearly all of these use the default text, only changing
    the name, address and email address. To find those petitions which
    have been modified, sorting by distance from the centroid gives
    emails where the default text has been completely replaced, added
    to, or has had portions removed, with the emails most heavily
    modified appearing first.
  </fo:block>
  <fo:block id="nlp.comparedoctocentroid">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.compareDocToCentroid</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">Cosine similarity of a document and a centroid,
      subtracting the document from the centroid</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax:
      <fo:inline font-family="Pragmata Pro">.nlp.compareDocToCentroid[centroid;document]</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where
    </fo:block>
    <fo:list-block provisional-distance-between-starts="0.3cm" provisional-label-separation="0.15cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          <fo:inline font-family="Pragmata Pro">centroid</fo:inline> is the sum of all documents in a
          cluster which is a dictionary
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">•</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          <fo:inline font-family="Pragmata Pro">document</fo:inline> is a document in a cluster which
          is a dictionary
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      returns the cosine similarity of the two documents as a float.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)petition:laycorpus where laycorpus[`subject] like "Demand Ken*"
q)centroid:sum petition`keywords
q).nlp.compareDocToCentroid[centroid]each petition`keywords
0.2374891 0.2308969 0.2383573 0.2797052 0.2817323 0.3103245 0.279753 0.2396462 0.3534717 0.369767
q)outliers:petition iasc .nlp.compareDocToCentroid[centroid]each petition`keywords
</fo:block>
  </fo:block>
  <fo:block id="searching">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">Searching</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Searching can be done using words, documents, or collections of
      documents as the query or dataset. To search for documents similar
      to a given document, you can represent all documents as feature
      vectors using TF-IDF, then compare the cosine similarity of the
      query document to those in the dataset and find the most similar
      documents, with the cosine similarity giving a relevance score.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      The following example searches using
      <fo:inline font-family="Pragmata Pro">.nlp.compareDocs</fo:inline> for the document most similar
      to the below email where former Enron CEO Jeff Skilling is
      discussing finding a new fire chief.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)queryemail:first jeffcorpus where jeffcorpus[`text] like "Fire Chief Committee*"  
q)-1 queryemail`text;
q)mostsimilar:jeffcorpus first 1_idesc .nlp.compareDocs[queryemail`keywords]each jeffcorpus`keywords

Select Comm AGENDA - Jan 25-Febr 1

Houston Fire Chief Selection Committee Members: Jeff Skilling - Chairperson, 
Troy Blakeney, Gerald Smith, Roel Campos and James Duke.

Congratulations selection committee members! We have a very important and 
exciting task ahead of us. 

On the agenda for the next week are two important items - (1) the Mayor's 
February 1 news conference announcing the Houston Fire Chief selection 
committee and its members; and (2) coordination of an action plan, which we 
should work out prior to the news conference.

News Conference specifics:
speakers - Mayor Brown and Jeff Skilling
in attendance - all selection committee members
location - Fire Station #6, 3402 Washington Ave.
date - Thursday, February 1, 2001
time - 2pm
duration - approximately 30 minutes

I'd like to emphasize that it would be ideal if all selection committee 
members were present at the news conference. 

I will need bios on each committee member emailed to me by close of business 
Monday, January 29, 2001. These bios will be attached to a press release the 
Mayor's Office is compiling.

Coordination of action plan:
Since we have only 1 week between now and the news conference, Jeff has 
proposed that he take a stab at putting together an initial draft. He will 
then email to all committee members for comments/suggestions and make changes 
accordingly. Hope this works for everyone - if not, give me a call 
(713)-345-4840.

Thanks,
Lisa
</fo:block>
  </fo:block>
</fo:block><fo:block id="explaining-similarities">
  <fo:block id="idp140219963632616" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Explaining similarities</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    For any pair of documents or centroids, the list of features can be
    sorted by how much they contribute to the similarity.
  </fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    This example compares two of former Enron CEO Jeff Skilling’s
    emails, both of which have in common the subject of selecting
    Houston’s next fire chief.
  </fo:block>
  <fo:block id="nlp.explainsimilarity">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.explainSimilarity</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.explainSimilarity[doc1;doc2]</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">doc1</fo:inline> and <fo:inline font-family="Pragmata Pro">doc2</fo:inline> are
      dictionaries consisting of their associated documents’ keywords,
      returns a dictionary of how much of the similarity score each
      token is responsible for.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)10#.nlp.explainSimilarity . jeffcorpus[`keywords]568 358
fire     | 0.2588778
roel     | 0.1456685
committee| 0.1298068
mayor    | 0.1295087
station  | 0.09342764
chief    | 0.06948782
select   | 0.04325209
important| 0.03838308
members  | 0.03530552
plan     | 0.02459828
</fo:block>
  </fo:block>
</fo:block><fo:block id="sentiment-analysis">
  <fo:block id="idp140219963639688" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Sentiment analysis</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    Using a pre-built model of the degrees of positive and negative
    sentiment for English words and emoticons, as well as parsing to
    account for negation, adverbs and other modifiers, sentences can be
    scored for their negative, positive and neutral sentiment. The model
    included has been trained on social-media messages.
  </fo:block>
  <fo:block id="nlp.sentiment">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.sentiment</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">Sentiment of a sentence</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.sentiment x</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is string or a list of strings, returns
      a dictionary or table containing the sentiment of the text.
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      An run of sentences from <fo:inline font-style="italic">Moby Dick</fo:inline>:
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.sentiment("Three cheers,men--all hearts alive!";"No,no! shame upon all cowards-shame upon them!")
compound   pos       neg       neu      
----------------------------------------
0.7177249  0.5996797 0         0.4003203
-0.8802318 0         0.6910529 0.3089471
</fo:block>
  </fo:block>
</fo:block><fo:block id="importing-and-parsing-mbox-files">
  <fo:block id="idp140219963647480" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Importing and parsing MBOX files</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    The MBOX file is the most common format for storing email messages
    on a hard drive. All the messages for each mailbox are stored as a
    single, long, text file in a string of concatenated e-mail messages,
    starting with the <fo:inline font-style="italic">From</fo:inline> header of the message.
    The NLP library allows the user to import these files and creates a
    kdb+ table.
  </fo:block>
  <fo:table-and-caption margin-right="36pt" margin-bottom="9pt"><fo:table font-size="9pt"><fo:table-header border-bottom-style="solid" border-bottom-width=".5pt" page-break-after="avoid"><fo:table-row><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            Column
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            Type
          </fo:block></fo:table-cell><fo:table-cell font-style="italic" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block linefeed-treatment="preserve" text-align="left">
            Content
          </fo:block></fo:table-cell></fo:table-row></fo:table-header><fo:table-body><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">sender</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            list of characters
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            The name and email address of the sender
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">to</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            list of characters
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            The name and email address of the reciever/recievers
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">date</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            timestamp
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            The date
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">subject</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            list of characters
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            The subject of the email
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">text</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            list of characters
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            The original text of the email
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">contentType</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            list of characters
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            The content type of the email
          </fo:block></fo:table-cell></fo:table-row><fo:table-row page-break-inside="avoid"><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            <fo:inline font-family="Pragmata Pro">payload</fo:inline>
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            list of characters or dictionaries
          </fo:block></fo:table-cell><fo:table-cell padding-top="3pt" padding-bottom="3pt" padding-left="5pt" padding-right="5pt"><fo:block text-align="left">
            The payload of the email
          </fo:block></fo:table-cell></fo:table-row></fo:table-body></fo:table></fo:table-and-caption>
  <fo:block id="nlp.loademails">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.loadEmails</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">An MBOX file as a table of parsed metadata</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.loadEmails x</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is a string of the filepath, returns a
      table.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)email:.nlp.email.getMbox["/home/kx/nlp/datasets/tdwg.mbox"]
q)cols email
`sender`to`date`subject`contentType`payload`text
</fo:block>
  </fo:block>
  <fo:block id="nlp.email.getgraph">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.email.getGraph</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">Graph of who emailed whom, with the number of times they
      emailed</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.email.getGraph x</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is a table (result from
      <fo:inline font-family="Pragmata Pro">.nlp.email.i.parseMbox</fo:inline>), returns a table of
      to-from pairing.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.email.getGraph[emails]

sender                           to                               volume
------------------------------------------------------------------------
Donald.Hobern@csiro.au           tdwg-img@lists.tdwg.org          1     
Donald.Hobern@csiro.au           tdwg@lists.tdwg.org              1     
Donald.Hobern@csiro.au           vchavan@gbif.org                 1     
RichardsK@landcareresearch.co.nz tdwg-img@lists.tdwg.org          1     
Robert.Morris@cs.umb.edu         Tdwg-tag@lists.tdwg.org          1     
Robert.Morris@cs.umb.edu         tdwg-img@lists.tdwg.org          1     
mdoering@gbif.org                lee@blatantfabrications.com      1     
mdoering@gbif.org                tdwg-img@lists.tdwg.org          1     
morris.bob@gmail.com             tdwg-img@lists.tdwg.org          1     
ram@cs.umb.edu                   RichardsK@landcareresearch.co.nz 1     
ram@cs.umb.edu                   tdwg-img@lists.tdwg.org          2     
ricardo@tdwg.org                 a.rissone@nhm.ac.uk              3     
ricardo@tdwg.org                 leebel@netspace.net.au           3     
ricardo@tdwg.org                 tdwg-img@lists.tdwg.org          3     
ricardo@tdwg.org                 tdwg-lit@lists.tdwg.org          3     
ricardo@tdwg.org                 tdwg-obs@lists.tdwg.org          3     
ricardo@tdwg.org                 tdwg-process@lists.tdwg.org      3     
ricardo@tdwg.org                 tdwg-tag@lists.tdwg.org          3     
ricardo@tdwg.org                 tdwg-tapir@lists.tdwg.org        3     
roger@tdwg.org                   Tdwg-img@lists.tdwg.org          1     
</fo:block>
  </fo:block>
</fo:block><fo:block id="parsing-emails-from-a-string-format">
  <fo:block id="idp140219963678408" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Parsing emails from a string format</fo:block>
  <fo:block id="nlp.email.parsemail">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.email.parseMail</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">Parses an email in string format</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.email.parseMail x</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is an email in a string format, returns
      a dictionary of the headers and content.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q)table:.nlp.email.parseMail emailString
q)table 
`headers`content
</fo:block>
  </fo:block>
</fo:block><fo:block id="useful-functions">
  <fo:block id="idp140219963683992" font-size="18pt" line-height="22pt" margin-right="36pt" page-break-before="always" space-after="60pt" text-align="left">Useful functions</fo:block>
  <fo:block line-height="16pt" space-after="9pt" text-align="justify">
    These are functions that extract elements of the text that can be
    applied to NLP algorithms, or that can help you with your analysis.
  </fo:block>
  <fo:block id="nlp.findtimes">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.findTimes</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">All the times in a document</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.findTimes x</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is a string, returns a general list:
    </fo:block>
    <fo:list-block provisional-distance-between-starts=".7cm" provisional-label-separation="0.3cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">1.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
<fo:block linefeed-treatment="preserve" line-height="16pt" space-after="9pt" text-align="left">time
</fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">2.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          text of the time (string)
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">3.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          start index (long)
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">4.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          index after the end index (long)
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.findTimes "I went to work at 9:00am and had a coffee at 10:20"
09:00:00.000 "9:00am" 18 24
10:20:00.000 "10:20"  45 50
</fo:block>
  </fo:block>
  <fo:block id="nlp.finddates">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.findDates</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">All the dates in a document</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.findDates x</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is a string
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      returns a general list:
    </fo:block>
    <fo:list-block provisional-distance-between-starts=".7cm" provisional-label-separation="0.3cm"><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">1.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          start date of the range
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">2.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          end date of the range
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">3.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          text of the range
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">4.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          start index of the date (long)
        </fo:block>
      </fo:list-item-body></fo:list-item><fo:list-item><fo:list-item-label end-indent="label-end()"><fo:block line-height="16pt">5.</fo:block></fo:list-item-label><fo:list-item-body start-indent="body-start()">
        <fo:block line-height="16pt" space-after="9pt" text-align="justify">
          index after the end index (long)
        </fo:block>
      </fo:list-item-body></fo:list-item></fo:list-block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.findDates "I am going on holidays on the 12/04/2018 to New York and come back on the 18.04.2018"
2018.04.12 2018.04.12 "12/04/2018" 30 40
2018.04.18 2018.04.18 "18.04.2018" 74 84
</fo:block>
  </fo:block>
  <fo:block id="nlp.getsentences">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.getSentences</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">A document partitioned into sentences.</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.getSentences x</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">x</fo:inline> is a dictionary or a table of document
      records or subcorpus, returns a list of strings.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
/finds the sentences in the first chapter of MobyDick
q) .nlp.getSentences corpus[0]
</fo:block>
  </fo:block>
  <fo:block id="nlp.loadtextfromdir">
    <fo:block font-weight="bold" font-size="14pt" line-height="17pt" margin-right="36pt" page-break-after="avoid" space-after="6pt" space-before="18pt" text-align="left">.nlp.loadTextFromDir</fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      <fo:inline font-style="italic">All the files in a directory, imported
      recursively</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Syntax: <fo:inline font-family="Pragmata Pro">.nlp.loadTextFromDir x</fo:inline>
    </fo:block>
    <fo:block line-height="16pt" space-after="9pt" text-align="justify">
      Where <fo:inline font-family="Pragmata Pro">x</fo:inline> the directory’s filepath as a string,
      returns a table of filenames, paths and texts.
    </fo:block>
    <fo:block font-family="Pragmata Pro" font-size="10pt" margin-bottom="12pt" margin-left="10pt" page-break-inside="avoid" white-space="pre">
q).nlp.loadTextFromDir["./datasets/maildir/skilling-j"]

fileName path                                           text                 ..
-----------------------------------------------------------------------------..
1.       :./datasets/maildir/skilling-j/_sent_mail/1.   "Message-ID: &lt;1461010..
10.      :./datasets/maildir/skilling-j/_sent_mail/10.  "Message-ID: &lt;1371054..
100.     :./datasets/maildir/skilling-j/_sent_mail/100. "Message-ID: &lt;47397.1..
101.     :./datasets/maildir/skilling-j/_sent_mail/101. "Message-ID: &lt;2486283..
</fo:block>
  </fo:block>
</fo:block></fo:flow></fo:page-sequence></fo:root>
